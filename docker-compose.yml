# Jetson Assistant — GPU Backend Services
#
# Start the VLM backend for your hardware:
#   docker compose up -d                      # starts vLLM + assistant
#   docker compose --profile thor-bf16 up -d  # Thor BF16 7B
#   docker compose --profile orin up -d       # AGX Orin 64GB
#
# Add GPU STT (optional):
#   docker compose --profile gpu-stt up -d
#
# Check readiness:
#   curl http://localhost:8001/v1/models
#
# Tip: Run "sudo sysctl -w vm.drop_caches=3" before starting containers
#       to reclaim leaked GPU memory from previous runs.

x-common: &common
  runtime: nvidia
  network_mode: host
  ipc: host
  restart: unless-stopped
  ulimits:
    memlock: -1
    stack: 67108864
  volumes:
    - hf-cache:/root/.cache/huggingface

volumes:
  hf-cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${HF_CACHE_DIR:-${HOME}/.cache/huggingface}

services:
  # ── VLM: Thor NVFP4 7B (default, fastest) ────────────────────────────
  # No profiles → starts with bare "docker compose up -d"
  vllm:
    <<: *common
    image: ghcr.io/nvidia-ai-iot/vllm:latest-jetson-thor
    container_name: jetson-assistant-vllm
    command: >
      vllm serve nvidia/Qwen2.5-VL-7B-Instruct-NVFP4
      --host 0.0.0.0 --port 8001
      --max-model-len 4096
      --gpu-memory-utilization 0.3
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8001/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 300s

  # ── VLM: Thor BF16 7B (best quality, slower vision) ──────────────────
  vllm-thor-bf16:
    <<: *common
    image: ghcr.io/nvidia-ai-iot/vllm:latest-jetson-thor
    container_name: jetson-assistant-vllm
    profiles: ["thor-bf16"]
    command: >
      vllm serve Qwen/Qwen2.5-VL-7B-Instruct
      --host 0.0.0.0 --port 8001
      --max-model-len 4096
      --limit-mm-per-prompt '{"image": 2}'
      --gpu-memory-utilization 0.3
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8001/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 300s

  # ── VLM: AGX Orin 64GB (BF16 7B, different container) ────────────────
  vllm-orin:
    <<: *common
    image: ghcr.io/nvidia-ai-iot/vllm:latest-jetson
    container_name: jetson-assistant-vllm
    profiles: ["orin"]
    command: >
      vllm serve Qwen/Qwen2.5-VL-7B-Instruct
      --host 0.0.0.0 --port 8001
      --max-model-len 4096
      --limit-mm-per-prompt '{"image": 2}'
      --gpu-memory-utilization 0.5
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8001/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 300s

  # ── GPU STT: vLLM Whisper (optional, ~126ms) ─────────────────────────
  whisper:
    <<: *common
    image: vllm-whisper-audio:latest
    container_name: jetson-assistant-whisper
    profiles: ["gpu-stt"]
    command: >
      vllm serve openai/whisper-large-v3-turbo
      --host 0.0.0.0 --port 8002
      --max-model-len 448
      --gpu-memory-utilization 0.1
      --enforce-eager
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8002/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s

  # ── Voice Assistant (Kokoro TTS + Nemotron STT, waits for vLLM) ────────
  assistant:
    image: ghcr.io/amarrmb/jetson-assistant:thor
    build: .
    <<: *common
    container_name: jetson-assistant
    devices:
      - /dev/snd    # ALSA audio I/O (mic + speaker)
    volumes:
      - hf-cache:/root/.cache/huggingface
    environment:
      - TORCH_COMPILE_DISABLE=1
    depends_on:
      vllm:
        condition: service_healthy
