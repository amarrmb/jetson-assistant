# Jetson AGX Orin â€” Natural UX, Vision Capable
# 3B VLM + Voice Assistant (Kokoro TTS + Nemotron STT)
#
# Usage: docker compose -f docker-compose.orin.yml up -d
#
# Check readiness:
#   curl http://localhost:8001/v1/models
#
# Tip: Run "sudo sysctl -w vm.drop_caches=3" before starting containers
#       to reclaim leaked GPU memory from previous runs.

x-common: &common
  runtime: nvidia
  network_mode: host
  ipc: host
  restart: unless-stopped
  ulimits:
    memlock: -1
    stack: 67108864

volumes:
  hf-cache:

services:
  vllm:
    <<: *common
    image: ghcr.io/nvidia-ai-iot/vllm:latest-jetson
    container_name: assistant-vllm
    volumes:
      - hf-cache:/root/.cache/huggingface
    command: >
      vllm serve Qwen/Qwen2.5-VL-3B-Instruct
      --host 0.0.0.0 --port 8001
      --max-model-len 4096
      --gpu-memory-utilization 0.4
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8001/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 300s

  assistant:
    <<: *common
    image: ghcr.io/amarrmb/jetson-assistant:orin
    container_name: assistant
    devices:
      - /dev/snd
    device_cgroup_rules:
      - 'c 81:* rmw'
    volumes:
      - /dev:/dev
      - hf-cache:/root/.cache/huggingface
      - /usr/lib/aarch64-linux-gnu:/host-libs:ro
      - /usr/local/cuda:/usr/local/cuda:ro
    environment:
      - TORCH_COMPILE_DISABLE=1
      - LD_LIBRARY_PATH=/usr/lib/aarch64-linux-gnu:/host-libs:/host-libs/libcudss/13:/usr/local/cuda/lib64:/usr/local/cuda/compat
      - ALSA_CARD=${ALSA_CARD:-}
    depends_on:
      vllm:
        condition: service_healthy
    command: ["assistant", "--config", "configs/orin.yaml"]
